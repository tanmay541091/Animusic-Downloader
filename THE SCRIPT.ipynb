{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f33b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "from xml.etree import ElementTree as Xet\n",
    "import csv\n",
    "import pandas as pd\n",
    "import urllib3\n",
    "import requests\n",
    "#pip install requests beautifulsoup4 \n",
    "import bs4\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "#https://github.com/LaurenceRawlings/savify\n",
    "#pip install -U savify\n",
    "from savify import Savify\n",
    "from savify.types import Type, Format, Quality\n",
    "from savify.utils import PathHolder\n",
    "from savify.logger import Logger\n",
    "\n",
    "#https://github.com/alexmercerind/youtube-search-python\n",
    "#pip install youtube-search-python\n",
    "import youtubesearchpython as yts\n",
    "\n",
    "#https://github.com/ytdl-org/youtube-dl#embedding-youtube-dl\n",
    "#https://stackoverflow.com/questions/32482230/how-to-set-up-default-download-location-in-youtube-dl\n",
    "import youtube_dl\n",
    "import os\n",
    "\n",
    "#unused\n",
    "#pip install music-tag\n",
    "#%pip install pytube\n",
    "#from pytube import YouTube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9818e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Module 1: Playing with html pages\n",
    "#converts the xml page into a dataframe, filters based on watching/completed and also adds mal link in last column\n",
    "def MakeCSV(exportedxml):\n",
    "    xml = Xet.parse(exportedxml)\n",
    "    csvfile = open('mal_to_csv.csv','w',encoding='utf-8')\n",
    "    csvfile_writer = csv.writer(csvfile)\n",
    "    csvfile_writer.writerow(['AnimeTitle','ID','Status'])\n",
    "\n",
    "    for anime in xml.findall('anime'):\n",
    "        if(anime):\n",
    "            animename = anime.find('series_title')\n",
    "            animeid = anime.find('series_animedb_id')\n",
    "            animestatus = anime.find('my_status')\n",
    "            csv_line = [animename.text, animeid.text, animestatus.text]\n",
    "            csvfile_writer.writerow(csv_line)\n",
    "    csvfile.close()  \n",
    "\n",
    "    df = pd.read_csv('mal_to_csv.csv')\n",
    "    df2= df.dropna()\n",
    "    filters = ['Completed', 'Watching']\n",
    "    df3 = df2[df2['Status'].isin(['Completed', 'Watching'])].reset_index()\n",
    "    del df3['index']\n",
    "    def make_url(row):\n",
    "        return 'https://myanimelist.net/anime/' + str(row[1])\n",
    "    df3['URL'] = df3.apply(make_url, axis = 1)\n",
    "    return df3\n",
    "\n",
    "#this function gives the title, season and year of the anime as a list\n",
    "def AnimeAndSeason(entry_page):\n",
    "    #title  = entry_page.title.text.replace('\\n', ' ').strip(' MyAnimeList.net')\n",
    "    #title  = title.strip(' -')\n",
    "    title  = entry_page.title.text.replace('\\n', ' ').split(' - ')[0].strip()\n",
    "    links  = entry_page.select('span > a')\n",
    "    years  = range(1900,2050)\n",
    "    season = ''\n",
    "    year   = 1900\n",
    "    for row in links:\n",
    "        test = row.contents[0].split()\n",
    "        if len(test) == 2 and test[1].isdigit() and int(test[1]) in years:\n",
    "            season_info = test\n",
    "            season = season_info[0] + ' ' + season_info[1]\n",
    "            year = int(season_info[1])\n",
    "    return(title, season, year)\n",
    "\n",
    "#function below gives a lil bit fucked list of the category/song/artist for individual mal page\n",
    "def SongAndArtist(entry_page):\n",
    "    testing = entry_page.find_all('div', attrs={'class':'di-tc va-t'})\n",
    "    testing2 = entry_page.find_all('div', attrs={'class':'di-tc va-t borderDark pb4'})\n",
    "    testing3 = testing + testing2\n",
    "    SAAlist = []\n",
    "    for entry in testing3:\n",
    "        category = entry.h2\n",
    "        if type(category) == bs4.element.Tag: \n",
    "            category_name = category.contents\n",
    "            SAAlist.append(category_name)\n",
    "\n",
    "        song_td_set = entry.find_all('td', {'width':'84%'})\n",
    "        for song_td in song_td_set:\n",
    "            song_data = song_td.text.replace('\\n', ' ').strip()\n",
    "            row = song_data.split('\\xa0') #split song# and song-artist\n",
    "            for datas in row:\n",
    "                if ' by ' in datas:\n",
    "                    row2 = datas.split(' by ') #split song and artist\n",
    "                    data_entry = [row2[0].strip('\\\"'), row2[1]]  #join song, artist\n",
    "                    song_links = song_td.find_all('input')\n",
    "                    for link in song_links:\n",
    "                        somelink = link['value']\n",
    "                        if 'spotify' in somelink:\n",
    "                            data_entry.append(somelink)\n",
    "                    SAAlist.append(data_entry)\n",
    "    return SAAlist\n",
    "\n",
    "#this combines the previous 2 functions and gives a nice dataframe of each row having all the identifying factors of a song\n",
    "def CreateSongDF(entry_page):\n",
    "    aas = AnimeAndSeason(entry_page)\n",
    "    saa = SongAndArtist(entry_page)\n",
    "    if aas[0] == 'Gyo (GYO: Tokyo Fish Attack!)': saa[2][0] = 'End Theme'\n",
    "        \n",
    "    for line in saa:\n",
    "        if 'Opening Theme' in line: op_index = saa.index(line)\n",
    "        if 'Ending Theme' in line: ed_index = saa.index(line)\n",
    "            \n",
    "    OP = []\n",
    "    category = saa[op_index][0]\n",
    "    for op in range(op_index+1,ed_index):\n",
    "        temp = saa[op]\n",
    "        temp.insert(0,category)\n",
    "        OP.append(temp)\n",
    "\n",
    "    ED = []\n",
    "    category = saa[ed_index][0]\n",
    "    for ed in range(ed_index+1,len(saa)):\n",
    "        temp = saa[ed]\n",
    "        temp.insert(0,category)\n",
    "        ED.append(temp)\n",
    "    \n",
    "    def Clean_Blanks(List, Length):\n",
    "        for item in List:\n",
    "            if len(item) != Length:\n",
    "                item.extend(' ')\n",
    "       \n",
    "    Clean_Blanks(ED, 4)\n",
    "    Clean_Blanks(OP, 4)\n",
    "    song_columns = ['category', 'songname', 'artist', 'link']\n",
    "    opdb = pd.DataFrame(OP, columns = song_columns)\n",
    "    eddb = pd.DataFrame(ED, columns = song_columns)\n",
    "    SongDB = pd.concat([opdb,eddb], axis = 0, ignore_index = True)\n",
    "    SongDB['extra'] = ''\n",
    "    return SongDB\n",
    "\n",
    "#Module 2: Playing with internet for spotify and youtube links\n",
    "#this creates a new dataframe (withc columns as song metadata and a 'links' column) from the csv containing mal urls\n",
    "def GetSpotify(malcsv):\n",
    "    song_table = pd.read_csv(malcsv).reset_index()\n",
    "    song_table.drop(['Unnamed: 0', 'index'], inplace=True, axis=1)    \n",
    "    link_table = pd.DataFrame(columns = ['anime', 'season', 'year','category', 'songname', 'artist', 'link', 'extra'])\n",
    "    for i in song_table.index:\n",
    "        anime_link = song_table.loc[i,'URL']\n",
    "        html       = requests.get(anime_link).text #type: ignore       \n",
    "        mal_page   = bs(html, 'lxml')  \n",
    "        aas        = AnimeAndSeason(mal_page)\n",
    "\n",
    "        song_subtable    = CreateSongDF(mal_page)\n",
    "        anime_subtable   = pd.DataFrame(columns=['anime', 'season', 'year'])\n",
    "        print('Fetching spotify links for', aas[0])\n",
    "        \n",
    "        for i in range(0, len(song_subtable)):\n",
    "            anime_subtable.loc[i,'anime'], anime_subtable.loc[i,'season'],  anime_subtable.loc[i,'year']   = aas[0], aas[1], aas[2]\n",
    "        anime_subtable = pd.concat([anime_subtable, song_subtable], axis = 1)\n",
    "        link_table     = pd.concat([link_table, anime_subtable], axis = 0)\n",
    "    link_table = link_table.reset_index()\n",
    "    del link_table['index']\n",
    "    return link_table\n",
    "\n",
    "#update the spotify sheet with youtube links as well to get a final sheet with all links\n",
    "def GetYoutube(spotcsv):\n",
    "    links.to_csv(spotcsv)\n",
    "    del link_table['Unnamed: 0']\n",
    "    for j in link_table.index:\n",
    "        if 'spotify' in str(link_table.loc[j,'link']) or 'youtube' in str(link_table.loc[j,'link']): continue \n",
    "\n",
    "        print(j,'Fetching youtube links for', link_table.loc[j,'anime'],link_table.loc[j,'songname'])\n",
    "        search_text = link_table.loc[j,'songname'] + ' ' + link_table.loc[j,'artist']\n",
    "        \n",
    "        if link_table.loc[j,'extra'] == 'fuckup': search_text = link_table.loc[j,'anime'] + ' ' + link_table.loc[j,'songname']\n",
    "        if link_table.loc[j,'extra'] == 'fuckup2': search_text = link_table.loc[j,'songname']\n",
    "\n",
    "        first_result = yts.VideosSearch(search_text, limit = 1).result()\n",
    "        if first_result['result'] == []: \n",
    "            if link_table.loc[j,'extra'] == 'fuckup': link_table.loc[j,'extra'] = 'fuckup2'\n",
    "            link_table.loc[j,'extra'] = 'fuckup'\n",
    "            print('youtube search fuckup #')\n",
    "            continue\n",
    "            link_table.loc[j,'link'] =  first_result['result'][0]['link']\n",
    "            link_table.loc[j,'extra'] =  first_result['result'][0]['title']\n",
    "    link_table['status'] = ''\n",
    "    return link_table\n",
    "\n",
    "#function to make all entries in the databases valid filenames\n",
    "def ValidFile(txt):\n",
    "    filename = ''\n",
    "    if '...' in txt:\n",
    "        txt = txt.strip('...')\n",
    "    for i in txt:\n",
    "        if i not in '\\/*?<>|\"':\n",
    "            filename = filename + i\n",
    "        elif i in ':':\n",
    "            filename = filename + '-'\n",
    "        else:\n",
    "            filename = filename + '_'\n",
    "    return filename\n",
    "\n",
    "#function to check differences in old and new links, replace links if new link found\n",
    "def New_Entries(new, old):\n",
    "    temp = pd.DataFrame(columns = old.columns.tolist())\n",
    "    for i in new.index:\n",
    "        link, song, artist  = new.loc[i, 'link'], new.loc[i, 'songname'], new.loc[i, 'artist']   \n",
    "        for j in old.index:\n",
    "            if old.loc[j, 'link'] ==  link:\n",
    "                link = \"BUSTED\"\n",
    "                break\n",
    "            if 'spotify' not in link and old.loc[j, 'songname'] == song and old.loc[j, 'artist'] == artist: \n",
    "                new.loc[i,'extra'] = 'yt upgrade to spot'\n",
    "    \n",
    "        if link != \"BUSTED\": \n",
    "            print(\"found a new link:\", link)\n",
    "            temp = temp.append(new.loc[i], ignore_index = True)\n",
    "    return temp\n",
    "\n",
    "#Module 3: Playing with external libraries to download songs\n",
    "#downloads spotify links into nested folder properly\n",
    "def DownloadSpotify(sheet_with_spotify, dl_loc, start, end):\n",
    "    songset = sheet_with_spotify.iloc[start:end]\n",
    "    for i in songset.index:\n",
    "        if 'spotify' not in songset.loc[i,'link'] or songset.loc[i,'status'] == 'Downloaded': continue\n",
    "        #https://developer.spotify.com/dashboard/applications/f2d89a80d48e41e19e4263d5b3792c21 for api credentials\n",
    "        sp_link = songset.loc[i,'link']\n",
    "        year    = str(songset.loc[i,'year'])\n",
    "        season  = str(songset.loc[i,'season'])\n",
    "        oped    = songset.loc[i,'category']\n",
    "        anime   = ValidFile(songset.loc[i,'anime'])\n",
    "        title   = ValidFile(songset.loc[i,'songname'].strip())\n",
    "        artist  = ValidFile(songset.loc[i,'artist'].strip())\n",
    "        path    = year + '/' + season + '/' + anime + '/' + oped + '/' + artist + '/'+ title\n",
    "        ffpath  = 'ffmpeg'\n",
    "        dlpath  = dl_loc +'/' + 'Animusic'\n",
    "        api1    = 'f2d89a80d48e41e19e4263d5b3792c21'\n",
    "        api2    = '12f6688e93504599b3d0403e8a4f2cc8'\n",
    "        logger  = Logger(log_location = dlpath, log_level = None)\n",
    "        print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "        print(i, path)\n",
    "        print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "        \n",
    "        #C:\\Users\\dell\\AppData\\Roaming\\Savify\\temp is where the temporary download goes\n",
    "        s = Savify(api_credentials  = (api1,api2), \n",
    "                    download_format = Format.MP3, \n",
    "                    ffmpeg_location = ffpath, \n",
    "                    skip_cover_art  = False, \n",
    "                    path_holder = PathHolder(downloads_path=dlpath), \n",
    "                    group   = path, \n",
    "                    quality = Quality.BEST,\n",
    "                    logger  = logger)\n",
    "        s.download(sp_link)\n",
    "        sheet_with_spotify.loc[i,'status'] = 'Downloaded'\n",
    "\n",
    "#downloads youtube video but sus \n",
    "def DownloadYoutube(sheet_with_youtube, dl_loc, start, end):\n",
    "    songset2 = sheet_with_youtube.loc[start:end,:]\n",
    "    for i in songset2.index:\n",
    "        if 'youtube' not in songset2.loc[i,'link'] or str(songset2.loc[i,'status']) == 'Downloaded' or str(songset2.loc[i,'status']) == 'sus': continue\n",
    "        year    = str(songset2.loc[i,'year'])\n",
    "        season  = str(songset2.loc[i,'season'])\n",
    "        anime   = songset2.loc[i,'anime']\n",
    "        oped    = songset2.loc[i,'category']\n",
    "        title   = songset2.loc[i,'songname']\n",
    "        artist  = songset2.loc[i,'artist']\n",
    "        yt_link = songset2.loc[i,'link']\n",
    "        path    = dl_loc + '/' + year + '/' + season + '/' + anime + '/' + oped + '/' + artist + '/'+ title + '/'\n",
    "        def my_hook(d):\n",
    "            if d['status'] == 'finished':\n",
    "                print('~~~~~~', i, title, artist,'~~~~~~')\n",
    "\n",
    "        ydl_opts = {\n",
    "            'format': 'bestaudio',\n",
    "            'outtmpl': path + '%(title)s.%(ext)s',\n",
    "            'postprocessors': [{\n",
    "                'key': 'FFmpegExtractAudio',\n",
    "                'preferredcodec': 'mp3',\n",
    "                'preferredquality': '192',}],\n",
    "                'progress_hooks': [my_hook]}\n",
    "\n",
    "        with youtube_dl.YoutubeDL(ydl_opts) as ydl:\n",
    "            ydl.download([yt_link])\n",
    "        sheet_with_youtube.loc[i,'status'] = 'Downloaded'\n",
    "    return sheet_with_youtube\n",
    "\n",
    "#Module 4: Playing with datasets to ensure youtube links are right\n",
    "#returns the 'dataset' set with indexes between 'start' and 'end' updated with the new 'status'\n",
    "def manual_status_update(dataset, start, end, status)\n",
    "    yt = pd.read_csv(dataset)\n",
    "    del yt['Unnamed: 0']\n",
    "    for i in range(start, end):\n",
    "        if yt.loc[i,'status'] == 'sus': continue\n",
    "        yt.loc[i,'status'] = status\n",
    "    return yt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83b9977",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##converting xml to a csv with eligible anime and their mal links\n",
    "\n",
    "MakeCSV('mal_export.xml').to_csv('mal_to_csv_filtered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e64055",
   "metadata": {},
   "outputs": [],
   "source": [
    "##creating a new csv of all songs and spotify links\n",
    "\n",
    "GetSpotify('mal_to_csv_filtered.csv').to_csv('sheet_w_spot.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eed8396",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##filling in gaps with youtube links\n",
    "\n",
    "yt_csv = GetYoutube('sheet_w_spot.csv')\n",
    "#yt_csv = GetYoutube('yt_whole_spot_whole.csv') #uncomment for reruns\n",
    "yt_csv.to_csv('sheet_w_spot_w_yt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54856d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "##downloading songs from spotify\n",
    "\n",
    "dl_spot = pd.read_csv('sheet_w_spot_w_yt.csv')\n",
    "#dl_spot = pd.read_csv('sheet_d_spot_w_yt.csv') #uncomment for reruns\n",
    "del dl_spot['Unnamed: 0']\n",
    "\n",
    "DownloadSpotify(dl_spot, 'F:/', 0, 0)\n",
    "dl_spot.to_csv('sheet_d_spot_w_yt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf9336a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##classifying problematic songs as sus\n",
    "yt = pd.read_csv('sheet_d_spot_w_yt.csv')\n",
    "del sheet['Unnamed: 0']\n",
    "\n",
    "yt = pd.DataFrame(columns = sheet.columns.tolist())\n",
    "for i in sheet.index:\n",
    "    if 'spotify' in sheet['link'][i]: continue\n",
    "    if  yt.loc[i, 'extra'] == 'fuckup': yt.loc[i, 'status'] = 'sus'\n",
    "\n",
    "    red_set = ['cover', 'Cover', 'Nightcore', 'nightcore', 'Instrumental', 'instrumental']\n",
    "    if any(red in yt.loc[i, 'extra'] for red in red_set): yt.loc[i, 'status'] = 'sus'\n",
    "   \n",
    "    red_index = [52,range(62,83),range(115,119),121,130,131,146,147,148,176,200,range(219,225),236,range(242,243),range(266,268),280,282,304,316,352,359,362,376,range(390,401),437,range(458,460),494,521,522,529,range(562,567),570,573,601,613,625,632,range(644,650),range(652,654),664,665]\n",
    "    #if i in red_index: yt.loc[i, 'status'] = 'sus'\n",
    "\n",
    "yt.to_csv('sheet_d_spot_s_yt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f92a916",
   "metadata": {},
   "outputs": [],
   "source": [
    "##downloading all songs from youtube links\n",
    "yt = pd.read_csv('sheet_d_spot_s_yt.csv')\n",
    "#dl_spot = pd.read_csv('sheet_d_spot_d_yt.csv') #uncomment for reruns\n",
    "del yt['Unnamed: 0']\n",
    "DownloadYoutube(yt, 'F:/Animusic', 0, 50)\n",
    "yt.to_csv('sheet_d_spot_d_yt.csv')\n",
    "\n",
    "#manual_status_update('sheet_d_spot_d_yt.csv', 0, 13, 'Downloaded').to_csv('sheet_d_spot_d_yt.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ee5a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Ensuring robustness for new database, i.e. everytime a new mal list is exported\n",
    "##How to introduce insert songs from sources other than mal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ace6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "yt = pd.read_csv('sheet_d_spot_w_yt_copy.csv')\n",
    "del yt['Unnamed: 0']\n",
    "i = 40\n",
    "yt.loc[i, 'link']  = 'https://www.youtube.com/watch?v=xYUzwwPIsnQ'\n",
    "yt.loc[i, 'extra'] = '五等分の軌跡'\n",
    "\n",
    "i = \n",
    "yt.loc[i, 'link'], yt.loc[i, 'extra'] = \n",
    "\n",
    "i = \n",
    "yt.loc[i, 'link'], yt.loc[i, 'extra'] = \n",
    "\n",
    "i = \n",
    "yt.loc[i, 'link'], yt.loc[i, 'extra'] = \n",
    "\n",
    "i = \n",
    "yt.loc[i, 'link'], yt.loc[i, 'extra'] = \n",
    "\n",
    "i = \n",
    "yt.loc[i, 'link'], yt.loc[i, 'extra'] = \n",
    "\n",
    "i = \n",
    "yt.loc[i, 'link'], yt.loc[i, 'extra'] = \n",
    "\n",
    "i = \n",
    "yt.loc[i, 'link'], yt.loc[i, 'extra'] = \n",
    "\n",
    "i = \n",
    "yt.loc[i, 'link'], yt.loc[i, 'extra'] = \n",
    "\n",
    "yt.to_csv('sheet_d_spot_w_yt_copy.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "f3a52463084db90f96d29dcfcfd9bf276dba3c521d76c4c38c835392b64a093b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
