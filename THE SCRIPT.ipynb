{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65f33b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "from xml.etree import ElementTree as Xet\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib3\n",
    "import requests\n",
    "#%pip install requests beautifulsoup4\n",
    "#%pip install lxml\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "#https://github.com/LaurenceRawlings/savify\n",
    "#%pip install -U savify\n",
    "from savify import Savify\n",
    "from savify.types import Type, Format, Quality\n",
    "from savify.utils import PathHolder\n",
    "from savify.logger import Logger\n",
    "\n",
    "#https://github.com/alexmercerind/youtube-search-python\n",
    "#%pip install youtube-search-python\n",
    "#import youtubesearchpython as yts\n",
    "\n",
    "#https://github.com/ytdl-org/youtube-dl#embedding-youtube-dl\n",
    "#https://stackoverflow.com/questions/32482230/how-to-set-up-default-download-location-in-youtube-dl\n",
    "import youtube_dl\n",
    "import os\n",
    "\n",
    "#unused\n",
    "#%pip install music-tag\n",
    "#%pip install pytube\n",
    "#from pytube import YouTube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9818e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module 1: Playing with html pages\n",
    "# Converts the xml page into a dataframe, filters based on watching/completed and also adds mal link in last column\n",
    "def MakeCSV(exportedxml):\n",
    "    xml = Xet.parse(exportedxml)\n",
    "    csvfile = open('mal_to_csv.csv','w',encoding='utf-8')\n",
    "    csvfile_writer = csv.writer(csvfile)\n",
    "    csvfile_writer.writerow(['AnimeTitle','ID','Status'])\n",
    "\n",
    "    for anime in xml.findall('anime'):\n",
    "        if(anime):\n",
    "            animename = anime.find('series_title')\n",
    "            animeid = anime.find('series_animedb_id')\n",
    "            animestatus = anime.find('my_status')\n",
    "            csv_line = [animename.text, animeid.text, animestatus.text]\n",
    "            csvfile_writer.writerow(csv_line)\n",
    "    csvfile.close()  \n",
    "\n",
    "    df = pd.read_csv('mal_to_csv.csv')\n",
    "    df2= df.dropna()\n",
    "    filters = ['Completed', 'Watching']\n",
    "    df3 = df2[df2['Status'].isin(['Completed', 'Watching'])].reset_index()\n",
    "    del df3['index']\n",
    "    def make_url(row):\n",
    "        return 'https://myanimelist.net/anime/' + str(row[1])\n",
    "    df3['URL'] = df3.apply(make_url, axis = 1)\n",
    "    return df3\n",
    "\n",
    "# This function gives the title, season and year of the anime as a list\n",
    "def AnimeAndSeason(entry_page):\n",
    "    title  = entry_page.title.text.replace('\\n', ' ').split(' - ')[0].strip()\n",
    "    links  = entry_page.select('span > a')\n",
    "    years  = range(1900,2050)\n",
    "    season = ''\n",
    "    year   = 1900\n",
    "    for row in links:\n",
    "        test = row.contents[0].split()\n",
    "        if len(test) == 2 and test[1].isdigit() and int(test[1]) in years:\n",
    "            season_info = test\n",
    "            season = season_info[0] + ' ' + season_info[1]\n",
    "            year = int(season_info[1])\n",
    "    return(title, season, year)\n",
    "\n",
    "# Function below gives a lil bit fucked list of the category/song/artist for individual mal page\n",
    "def SongAndArtist(entry_page):\n",
    "    testing = entry_page.find_all('div', attrs={'class':'di-tc va-t'})\n",
    "    testing2 = entry_page.find_all('div', attrs={'class':'di-tc va-t borderDark pb4'})\n",
    "    testing3 = testing + testing2\n",
    "    SAAlist = []\n",
    "    for entry in testing3:\n",
    "        category = entry.h2\n",
    "        if type(category) == bs4.element.Tag: \n",
    "            category_name = category.contents\n",
    "            SAAlist.append(category_name)\n",
    "\n",
    "        song_td_set = entry.find_all('td', {'width':'84%'})\n",
    "        for song_td in song_td_set:\n",
    "            song_data = song_td.text.replace('\\n', ' ').strip()\n",
    "            row = song_data.split('\\xa0') #split song# and song-artist\n",
    "            for datas in row:\n",
    "                if ' by ' in datas:\n",
    "                    row2 = datas.split(' by ') #split song and artist\n",
    "                    data_entry = [row2[0].strip('\\\"'), row2[1]]  #join song, artist\n",
    "                    song_links = song_td.find_all('input')\n",
    "                    for link in song_links:\n",
    "                        somelink = link['value']\n",
    "                        if 'spotify' in somelink:\n",
    "                            data_entry.append(somelink)\n",
    "                    SAAlist.append(data_entry)\n",
    "    return SAAlist\n",
    "\n",
    "# This combines the previous 2 functions and gives a nice dataframe of each row having all the identifying factors of a song\n",
    "def CreateSongDF(entry_page):\n",
    "    aas = AnimeAndSeason(entry_page)\n",
    "    saa = SongAndArtist(entry_page)\n",
    "    if aas[0] == 'Gyo (GYO: Tokyo Fish Attack!)': saa[2][0] = 'End Theme'\n",
    "    \n",
    "    for line in saa:\n",
    "        if 'Opening Theme' in line: op_index = saa.index(line)\n",
    "        if 'Ending Theme' in line: ed_index = saa.index(line)\n",
    "                   \n",
    "    OP = []\n",
    "    category = saa[op_index][0]\n",
    "    for op in range(op_index+1,ed_index):\n",
    "        temp = saa[op]\n",
    "        temp.insert(0,category)\n",
    "        OP.append(temp)\n",
    "\n",
    "    ED = []\n",
    "    category = saa[ed_index][0]\n",
    "    for ed in range(ed_index+1,len(saa)):\n",
    "        temp = saa[ed]\n",
    "        temp.insert(0,category)\n",
    "        ED.append(temp)\n",
    "    \n",
    "    def Clean_Blanks(List, Length):\n",
    "        for item in List:\n",
    "            if len(item) != Length:\n",
    "                item.extend(' ')\n",
    "       \n",
    "    Clean_Blanks(ED, 4)\n",
    "    Clean_Blanks(OP, 4)\n",
    "    song_columns = ['category', 'songname', 'artist', 'link']\n",
    "    opdb = pd.DataFrame(OP, columns = song_columns)\n",
    "    eddb = pd.DataFrame(ED, columns = song_columns)\n",
    "    SongDB = pd.concat([opdb,eddb], axis = 0, ignore_index = True)\n",
    "    SongDB['status'] = ''\n",
    "    return SongDB\n",
    "\n",
    "# Module 2: Playing with internet for spotify and youtube links\n",
    "# This creates a new dataframe (withc columns as song metadata and a 'links' column) from the csv containing mal urls\n",
    "def GetSpotify(malcsv, start, length):\n",
    "    song_table = pd.read_csv(malcsv).reset_index()\n",
    "    song_table.drop(['Unnamed: 0', 'index'], inplace=True, axis=1)\n",
    "    song_table = song_table.iloc[range(start, start+length), :]\n",
    "    link_table = pd.DataFrame(columns = ['anime', 'season', 'year','category', 'songname', 'artist', 'link', 'status'])\n",
    "    for i in song_table.index:\n",
    "        anime_link = song_table.loc[i,'URL']\n",
    "        html       = requests.get(anime_link).text #type: ignore       \n",
    "        mal_page   = bs(html, 'lxml')  \n",
    "        aas        = AnimeAndSeason(mal_page)\n",
    "\n",
    "        song_subtable    = CreateSongDF(mal_page)\n",
    "        anime_subtable   = pd.DataFrame(columns=['anime', 'season', 'year'])\n",
    "        print(i, 'Fetching spotify links for', aas[0])\n",
    "\n",
    "        for i in range(0, len(song_subtable)):\n",
    "            anime_subtable.loc[i,'anime'], anime_subtable.loc[i,'season'],  anime_subtable.loc[i,'year']   = aas[0], aas[1], aas[2]\n",
    "        anime_subtable = pd.concat([anime_subtable, song_subtable], axis = 1)\n",
    "        anime_subtable.to_csv('sheet_w_spot.csv', mode = 'a', index = False, header = False)\n",
    "\n",
    "# Function to make all entries in the databases valid filenames\n",
    "def ValidFile(txt):\n",
    "    filename = ''\n",
    "    if '...' in txt:\n",
    "        txt = txt.strip('...')\n",
    "    for i in txt:\n",
    "        if i not in '\\/*?<>|\"':\n",
    "            filename = filename + i\n",
    "        elif i in ':':\n",
    "            filename = filename + '-'\n",
    "        else:\n",
    "            filename = filename + '_'\n",
    "    return filename\n",
    "\n",
    "# Module 3: Playing with external libraries to download songs\n",
    "# Downloads spotify links into appropriate nested folder\n",
    "def DownloadSpotify(sheet_with_spotify, dl_loc, start, end):\n",
    "    songset = sheet_with_spotify.iloc[start:end]\n",
    "    for i in songset.index:\n",
    "        if 'spotify' not in songset.loc[i,'link'] or songset.loc[i,'status'] == 'Downloaded': continue\n",
    "        #https://developer.spotify.com/dashboard/applications/f2d89a80d48e41e19e4263d5b3792c21 for api credentials\n",
    "        sp_link = songset.loc[i,'link']\n",
    "        year    = str(songset.loc[i,'year'])\n",
    "        season  = str(songset.loc[i,'season'])\n",
    "        oped    = songset.loc[i,'category']\n",
    "        anime   = ValidFile(songset.loc[i,'anime'])\n",
    "        title   = ValidFile(songset.loc[i,'songname'].strip())\n",
    "        artist  = ValidFile(songset.loc[i,'artist'].strip())\n",
    "        path    = year + '/' + season + '/' + anime + '/' + oped + '/' + artist + '/'+ title\n",
    "        ffpath  = 'ffmpeg'\n",
    "        dlpath  = dl_loc +'/' + 'Animusic'\n",
    "        api1    = 'f2d89a80d48e41e19e4263d5b3792c21'\n",
    "        api2    = '12f6688e93504599b3d0403e8a4f2cc8'\n",
    "        logger  = Logger(log_location = dlpath, log_level = None)\n",
    "        print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "        print(i, path)\n",
    "        print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "        \n",
    "        #C:\\Users\\dell\\AppData\\Roaming\\Savify\\temp is where the temporary download goes\n",
    "        s = Savify(api_credentials  = (api1,api2), \n",
    "                    download_format = Format.MP3, \n",
    "                    ffmpeg_location = ffpath, \n",
    "                    skip_cover_art  = False, \n",
    "                    path_holder = PathHolder(downloads_path=dlpath), \n",
    "                    group   = path, \n",
    "                    quality = Quality.BEST,\n",
    "                    logger  = logger)\n",
    "        s.download(sp_link)\n",
    "        sheet_with_spotify.loc[i,'status'] = 'Downloaded'\n",
    "\n",
    "# Downloads youtube links into appropriate nested folder \n",
    "def DownloadYoutube(sheet_with_youtube, dl_loc, start, end):\n",
    "    songset2 = sheet_with_youtube.loc[start:end,:]\n",
    "    for i in songset2.index:\n",
    "        if 'youtube' not in songset2.loc[i,'link'] or str(songset2.loc[i,'status']) == 'Downloaded' or str(songset2.loc[i,'status']) == 'sus': continue\n",
    "        year    = str(songset2.loc[i,'year'])\n",
    "        season  = str(songset2.loc[i,'season'])\n",
    "        anime   = songset2.loc[i,'anime']\n",
    "        oped    = songset2.loc[i,'category']\n",
    "        title   = songset2.loc[i,'songname']\n",
    "        artist  = songset2.loc[i,'artist']\n",
    "        yt_link = songset2.loc[i,'link']\n",
    "        path    = dl_loc + '/' + year + '/' + season + '/' + anime + '/' + oped + '/' + artist + '/'+ title + '/'\n",
    "        def my_hook(d):\n",
    "            if d['status'] == 'finished':\n",
    "                print('~~~~~~', i, title, artist,'~~~~~~')\n",
    "\n",
    "        ydl_opts = {\n",
    "            'format': 'bestaudio',\n",
    "            'outtmpl': path + '%(title)s.%(ext)s',\n",
    "            'postprocessors': [{\n",
    "                'key': 'FFmpegExtractAudio',\n",
    "                'preferredcodec': 'mp3',\n",
    "                'preferredquality': '192',}],\n",
    "                'progress_hooks': [my_hook]}\n",
    "\n",
    "        with youtube_dl.YoutubeDL(ydl_opts) as ydl:\n",
    "            ydl.download([yt_link])\n",
    "        sheet_with_youtube.loc[i,'status'] = 'Downloaded'\n",
    "    return sheet_with_youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83b9977",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Converting xml to a csv with eligible anime and their mal links\n",
    "MakeCSV('mal_export.xml').to_csv('mal_to_csv_filtered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e64055",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating a new csv of all songs and spotify links, have to be done 150 at a time\n",
    "#  Also some links are broken so you will have to manually fix it \n",
    "link_table = pd.DataFrame(columns = ['anime', 'season', 'year','category', 'songname', 'artist', 'link', 'status'])\n",
    "GetSpotify('mal_to_csv_filtered.csv', 0, 150) # file, start, no of entries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "69ee5a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ensuring robustness for new database, i.e. everytime a new mal list is exported\n",
    "#  If a song is in song db, it's already dled, so need a code that will filter existing songs from new db, and then append the new songs after dling\n",
    "old = pd.read_csv('Older/sheet_d_spot_w_yt_copy.csv')\n",
    "new = pd.read_csv('sheet_w_spot.csv')\n",
    "new = new.replace(' ', pd.NA)\n",
    "old.drop('extra', inplace = True, axis = 1)\n",
    "\n",
    "#filtering duplicates in new df\n",
    "new1 = new[new['link'].isna()]\n",
    "new2 = new[new[\"link\"] != \"\"].drop_duplicates(subset=[\"link\"])\n",
    "new3 = pd.concat([new1, new2], axis=0).sort_index()\n",
    "new = new3.drop_duplicates(subset=[\"songname\", \"artist\", \"category\"])\n",
    "\n",
    "for index, row in new.iterrows():\n",
    "    # Check if the combination of anime, songname, and artist match in the old DataFrame\n",
    "    match_condition = (old['anime'] == row['anime']) & (old['songname'] == row['songname']) & (old['artist'] == row['artist'])\n",
    "\n",
    "    # Check if the link is different in the new DataFrame\n",
    "    if match_condition.any() and pd.isnull(row['link']) == 0 and old.loc[match_condition, 'link'].values[0] != row['link']:\n",
    "        # Update the link in the old DataFrame\n",
    "        old.loc[match_condition, 'link'] = row['link']\n",
    "\n",
    "        # Check if the status column in the old DataFrame contains \"Downloaded\" and replace it with blank\n",
    "        if old.loc[match_condition, 'status'].values[0] == \"Downloaded\":\n",
    "            old.loc[match_condition, 'status'] = 'SEX'\n",
    "\n",
    "del old['Unnamed: 0']\n",
    "old.to_csv('Wecooked.csv')\n",
    "#https://open.spotify.com/track/47mtXCSExRrKA9cYGuiftZ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "553dd85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 100)\n",
    "old.head(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54856d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Downloading songs from spotify\n",
    "#  Ensure you've added all new links manually as well. \n",
    "dl_spot = pd.read_csv('sheet_w_spot.csv')\n",
    "#dl_spot = pd.read_csv('sheet_d_spot_w_yt.csv') #uncomment for reruns\n",
    "del dl_spot['Unnamed: 0']\n",
    " \n",
    "DownloadSpotify(dl_spot, 'D:/_Animusic/', 0, 0)\n",
    "dl_spot.to_csv('sheet_d_spot_w_yt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f92a916",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Downloading all songs from youtube links\n",
    "yt = pd.read_csv('sheet_d_spot_s_yt.csv')\n",
    "#dl_spot = pd.read_csv('sheet_d_spot_d_yt.csv') #uncomment for reruns\n",
    "del yt['Unnamed: 0']\n",
    "DownloadYoutube(yt, 'F:/Animusic', 0, 50)\n",
    "yt.to_csv('sheet_d_spot_d_yt.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "22354d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leaderboard data has been appended to leaderboard_data.csv at 2023-10-04 10:42:24.\n"
     ]
    }
   ],
   "source": [
    "#pokefarm \n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "# Initialize a dictionary to store user scores\n",
    "user_scores = {}\n",
    "\n",
    "# Function to scrape data and append to CSV\n",
    "def scrape_and_append_to_csv():\n",
    "    payload = {'inUserName': 'tanmayonnaise', 'inUserPass': '534519B4C10C03F2'}\n",
    "    url = 'https://pokefarm.com/stats/interactions'\n",
    "    html = requests.get(url, data=payload) #type: ignore\n",
    "\n",
    "    # Parse the HTML using BeautifulSoup\n",
    "    soup = BeautifulSoup(html.content, 'html.parser')\n",
    "\n",
    "    # Find all the <li> elements within the <ol> with class \"leaderboard\"\n",
    "    leaderboard_items = soup.find_all('li')\n",
    "\n",
    "    # Get the current timestamp\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    # Loop through the <li> elements to update user scores\n",
    "    for item in leaderboard_items:\n",
    "        # Extract the user's name from the <a> element, accounting for both classes\n",
    "        user_link = item.find('a', class_='userlink0') or item.find('a', class_='userlink8')\n",
    "        user = user_link.text.strip() if user_link else \"\"\n",
    "\n",
    "        # Extract the user's score from the <span> element\n",
    "        score = item.find('span', class_='score').text.strip()\n",
    "\n",
    "        # Update the user's score in the dictionary\n",
    "        user_scores[user] = score\n",
    "\n",
    "    # Append the timestamp as a new column\n",
    "    user_scores[\"Timestamp\"] = timestamp\n",
    "\n",
    "    # Create a DataFrame from the dictionary\n",
    "    df = pd.DataFrame([user_scores])\n",
    "\n",
    "    # Define the CSV file name\n",
    "    csv_file_name = \"leaderboard_data.csv\"\n",
    "\n",
    "    # Append the data to the existing CSV file, or create a new file if it doesn't exist\n",
    "    df.to_csv(csv_file_name, mode='a', header=False, index=False)\n",
    "    \n",
    "    print(f\"Leaderboard data has been appended to {csv_file_name} at {timestamp}.\")\n",
    "\n",
    "# Run the code to scrape and append data to CSV\n",
    "scrape_and_append_to_csv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c706d85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "f3a52463084db90f96d29dcfcfd9bf276dba3c521d76c4c38c835392b64a093b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
