{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f33b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.etree import ElementTree as Xet\n",
    "import csv\n",
    "import pandas as pd\n",
    "import urllib3\n",
    "import requests\n",
    "#pip install requests beautifulsoup4 \n",
    "import bs4\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "#https://github.com/LaurenceRawlings/savify\n",
    "#pip install -U savify\n",
    "from savify import Savify\n",
    "from savify.types import Type, Format, Quality\n",
    "from savify.utils import PathHolder\n",
    "from savify.logger import Logger\n",
    "\n",
    "#https://github.com/alexmercerind/youtube-search-python\n",
    "#pip install youtube-search-python\n",
    "import youtubesearchpython as yts\n",
    "\n",
    "#https://github.com/ytdl-org/youtube-dl#embedding-youtube-dl\n",
    "#https://stackoverflow.com/questions/32482230/how-to-set-up-default-download-location-in-youtube-dl\n",
    "from __future__ import unicode_literals\n",
    "import youtube_dl\n",
    "import os\n",
    "\n",
    "#yet to use/unused\n",
    "#pip install music-tag\n",
    "#%pip install pytube\n",
    "#from pytube import YouTube"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9818e3f",
   "metadata": {},
   "source": [
    "#converts the xml page into a dataframe, filters based on watching/completed and also adds mal link in last column\n",
    "def MakeCSV(exportedxml):\n",
    "    xml = Xet.parse(exportedxml)\n",
    "    csvfile = open('mal_to_csv.csv','w',encoding='utf-8')\n",
    "    csvfile_writer = csv.writer(csvfile)\n",
    "    csvfile_writer.writerow(['AnimeTitle','ID','Status'])\n",
    "\n",
    "    for anime in xml.findall('anime'):\n",
    "        if(anime):\n",
    "            animename = anime.find('series_title')\n",
    "            animeid = anime.find('series_animedb_id')\n",
    "            animestatus = anime.find('my_status')\n",
    "            csv_line = [animename.text, animeid.text, animestatus.text]\n",
    "            csvfile_writer.writerow(csv_line)\n",
    "    csvfile.close()  \n",
    "\n",
    "    df = pd.read_csv('mal_to_csv.csv')\n",
    "    df2= df.dropna()\n",
    "    filters = ['Completed', 'Watching']\n",
    "    df3 = df2[df2['Status'].isin(['Completed', 'Watching'])].reset_index()\n",
    "    del df3['index']\n",
    "    def make_url(row):\n",
    "        return 'https://myanimelist.net/anime/' + str(row[1])\n",
    "    df3['URL'] = df3.apply(make_url, axis = 1)\n",
    "    return df3\n",
    "\n",
    "#this function gives the title, season and year of the anime as a list\n",
    "def AnimeAndSeason(entry_page):\n",
    "    title  = entry_page.title.text.replace('\\n', ' ').strip(' MyAnimeList.net')\n",
    "    title  = title.strip(' -')\n",
    "    links  = entry_page.select('span > a')\n",
    "    years  = range(1900,2050)\n",
    "    season_years = range(2018,2050)\n",
    "    season = ''\n",
    "    year   = 1900\n",
    "    for row in links:\n",
    "        test = row.contents[0].split()\n",
    "        if len(test) == 2 and test[1].isdigit() and int(test[1]) in years:\n",
    "            season_info = test\n",
    "            #if int(test[1]) in years:  can add this filter if dont want seasons for seasons before 2018\n",
    "            season = season_info[0] + ' ' + season_info[1]\n",
    "            year = int(season_info[1])\n",
    "    return(title, season, year)\n",
    "\n",
    "#function below gives a lil bit fucked list of the category/song/artist for individual mal page\n",
    "def SongAndArtist(entry_page):\n",
    "    testing = entry_page.find_all('div', attrs={'class':'di-tc va-t'})\n",
    "    testing2 = entry_page.find_all('div', attrs={'class':'di-tc va-t borderDark pb4'})\n",
    "    testing3 = testing + testing2\n",
    "    SAAlist = []\n",
    "    for entry in testing3:\n",
    "        category = entry.h2\n",
    "        if type(category) == bs4.element.Tag: \n",
    "            category_name = category.contents\n",
    "            SAAlist.append(category_name)\n",
    "\n",
    "        song_td_set = entry.find_all('td', {'width':'84%'})\n",
    "        for song_td in song_td_set:\n",
    "            song_data = song_td.text.replace('\\n', ' ').strip()\n",
    "            row = song_data.split('\\xa0') #split song# and song-artist\n",
    "            for datas in row:\n",
    "                if ' by ' in datas:\n",
    "                    row2 = datas.split(' by ') #split song and artist\n",
    "                    data_entry = [row2[0].strip('\\\"'), row2[1]]  #join song, artist\n",
    "                    song_links = song_td.find_all('input')\n",
    "                    for link in song_links:\n",
    "                        somelink = link['value']\n",
    "                        if 'spotify' in somelink:\n",
    "                            data_entry.append(somelink)\n",
    "                    SAAlist.append(data_entry)\n",
    "    return SAAlist\n",
    "\n",
    "#this combines the previous 2 functions and gives a nice dataframe of each row having all the identifying factors of a song\n",
    "def OPEDDF(entry_page):\n",
    "    aas = AnimeAndSeason(entry_page)\n",
    "    saa = SongAndArtist(entry_page)\n",
    "    if aas[0] == 'Gyo (GYO: Tokyo Fish Attack!)':\n",
    "        saa[2][0] = 'End Theme'\n",
    "        \n",
    "    for line in saa:\n",
    "        if 'Opening Theme' in line:\n",
    "            op_index = saa.index(line)\n",
    "        if 'Ending Theme' in line:\n",
    "            ed_index = saa.index(line)\n",
    "            \n",
    "    OP = []\n",
    "    category = saa[op_index][0]\n",
    "    for op in range(op_index+1,ed_index):\n",
    "        temp = saa[op]\n",
    "        temp.insert(0,category)\n",
    "        OP.append(temp)\n",
    "\n",
    "    ED = []\n",
    "    category = saa[ed_index][0]\n",
    "    for ed in range(ed_index+1,len(saa)):\n",
    "        temp = saa[ed]\n",
    "        temp.insert(0,category)\n",
    "        ED.append(temp)\n",
    "    \n",
    "    def Clean_Blanks(List, Length):\n",
    "        for item in List:\n",
    "            if len(item) != Length:\n",
    "                item.extend(' ')\n",
    "       \n",
    "    Clean_Blanks(ED, 4)\n",
    "    Clean_Blanks(OP, 4)\n",
    "    song_columns = ['category', 'songname', 'artist', 'spotify']\n",
    "    opdb = pd.DataFrame(OP, columns = song_columns)\n",
    "    eddb = pd.DataFrame(ED, columns = song_columns)\n",
    "    SongDB = pd.concat([opdb,eddb], axis = 0, ignore_index = True)\n",
    "    return SongDB\n",
    "\n",
    "#this creates a new (exportable) database from the csv containing mal urls \n",
    "#new database has direct spotify links as a column with all its identifiers as other columns\n",
    "#THIS IS THE FUNCTION ACCESSING INTERNET\n",
    "#as a precaution, only the head of input file is used for now, because mass pings to the internet\n",
    "def GetSpotify(songcsv):\n",
    "    song_table = pd.read_csv(songcsv)\n",
    "    song_table = song_table.reset_index()\n",
    "    del song_table['Unnamed: 0']\n",
    "    del song_table['index']\n",
    "    \n",
    "    df = song_table\n",
    "    \n",
    "    songset = pd.DataFrame(columns=['anime', 'season', 'year','category', \n",
    "                                    'songname', 'artist', 'spotify'])\n",
    "    for i in df.index:\n",
    "        animelink = df.loc[i,'URL']\n",
    "        html      = requests.get(animelink).text\n",
    "        mal_page  = bs(html, 'lxml')  \n",
    "        aas       = AnimeAndSeason(mal_page)   \n",
    "        songdb    = OPEDDF(mal_page)\n",
    "        tempset   = pd.DataFrame(columns=['anime', 'season', 'year'])\n",
    "        \n",
    "        print(aas[0])\n",
    "        for i in range(0, len(songdb)):\n",
    "            tempset.loc[i,'anime']  = aas[0]\n",
    "            tempset.loc[i,'season'] = aas[1]\n",
    "            tempset.loc[i,'year']   = aas[2]\n",
    "        tempset = pd.concat([tempset, songdb], axis = 1)\n",
    "        songset = pd.concat([songset, tempset], axis = 0)\n",
    "    songset = songset.reset_index()\n",
    "    del songset['index']\n",
    "    return songset\n",
    "\n",
    "#function to make all entries in the databases valid filenames\n",
    "def ValidFile(txt):\n",
    "    filename = ''\n",
    "    for i in txt:\n",
    "        #if i == 'ï¼‹': #for 22/7 ova\n",
    "            #filename = filename + ' plus '\n",
    "        if i not in '\\/:*?<>|\"':\n",
    "            filename = filename + i\n",
    "        else:\n",
    "            filename = filename + '_'\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a7a00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#downloads spotify links into nested folder properly\n",
    "def DownloadSpotify(sheet_with_spotify, dl_loc, end_point):\n",
    "    songset = sheet_with_spotify.head(end_point)\n",
    "    for i in songset.index:\n",
    "        if songset.loc[i,'status'] == 'Downloaded':\n",
    "            continue\n",
    "        sp_link = songset.loc[i,'spotify']\n",
    "        year    = str(songset.loc[i,'year'])\n",
    "        season  = str(songset.loc[i,'season'])\n",
    "        anime   = songset.loc[i,'anime']\n",
    "        oped    = songset.loc[i,'category']\n",
    "        title   = songset.loc[i,'songname']\n",
    "        artist  = songset.loc[i,'artist'].strip()\n",
    "        path    = year + '/' + season + '/' + anime + '/' + oped + '/' + artist + '/'+ title\n",
    "        ffpath  = 'ffmpeg'\n",
    "        dlpath  = dl_loc +'/' + 'Animusic'\n",
    "        #https://developer.spotify.com/dashboard/applications/f2d89a80d48e41e19e4263d5b3792c21\n",
    "        api1    = 'f2d89a80d48e41e19e4263d5b3792c21'\n",
    "        api2    = '12f6688e93504599b3d0403e8a4f2cc8'\n",
    "        logger  = Logger(log_location = dlpath, log_level = None)\n",
    "        print(i, path)\n",
    "        \n",
    "        #C:\\Users\\dell\\AppData\\Roaming\\Savify\\temp is where the temporary download goes\n",
    "        s = Savify(api_credentials  = (api1,api2), \n",
    "                    download_format = Format.MP3, \n",
    "                    ffmpeg_location = ffpath, \n",
    "                    skip_cover_art  = False, \n",
    "                    path_holder = PathHolder(downloads_path=dlpath), \n",
    "                    group   = path, \n",
    "                    quality = Quality.BEST,\n",
    "                    logger  = logger)\n",
    "        s.download(sp_link)\n",
    "        sheet_with_spotify.loc[i,'status'] = 'Downloaded'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc122fcf",
   "metadata": {},
   "source": [
    "#downloads youtube video, have to look into commands for metadata setup\n",
    "def DownloadYoutube(sheet_with_youtube, dl_loc):\n",
    "    songset2 = sheet_with_youtube.head()\n",
    "    for i in songset2.index:\n",
    "        year   = songset2.loc[i,'year']\n",
    "        season = songset2.loc[i,'season']\n",
    "        anime  = songset2.loc[i,'anime']\n",
    "        oped   = songset2.loc[i,'category']\n",
    "        title  = songset2.loc[i,'songname']\n",
    "        artist = songset2.loc[i,'artist']\n",
    "        path   = dl_loc +' / ' + 'Animusic' + '/' + str(year) + '/' + season + '/' + anime + '/' + oped + '/' + artist + '/'+ title + '/'\n",
    "\n",
    "        first_result = yts.VideosSearch(title + ' ' + artist, limit = 1)\n",
    "        yt_link      = first_result.result()['result'][0]['link']\n",
    "\n",
    "        def my_hook(d):\n",
    "            if d['status'] == 'finished':\n",
    "                print('Converting:', anime, title, artist)\n",
    "\n",
    "        ydl_opts = {\n",
    "            'format': 'bestaudio',\n",
    "            'outtmpl': path + '%(title)s - %(artist)s',\n",
    "            'postprocessors': [{\n",
    "                'key': 'FFmpegExtractAudio',\n",
    "                'preferredcodec': 'mp3',\n",
    "                'preferredquality': '192',}],\n",
    "                'progress_hooks': [my_hook]}\n",
    "\n",
    "        with youtube_dl.YoutubeDL(ydl_opts) as ydl:\n",
    "            ydl.download([yt_link])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83b9977",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#converting xml to a csv with eligible anime and their mal links\n",
    "filtered_mal = MakeCSV('mal_export.xml')\n",
    "filtered_mal.to_csv('mal_to_csv_filtered.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb1c035",
   "metadata": {},
   "source": [
    "#creating a new csv of all songs and spotify links\n",
    "sheet = GetSpotify('mal_to_csv_filtered.csv')\n",
    "sheet.to_csv('nice_looking_song_sheet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f068bb45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#initializing 2 dataframes, 1 with spotify links and 1 without\n",
    "sheet = pd.read_csv('nice_looking_song_sheet.csv')\n",
    "del sheet['Unnamed: 0']\n",
    "sws = sheet\n",
    "for i in sheet.index:\n",
    "    sws.loc[i,'anime']    = ValidFile(sws.loc[i,'anime'])\n",
    "    sws.loc[i,'songname'] = ValidFile(sws.loc[i,'songname'])\n",
    "    sws.loc[i,'artist']   = ValidFile(sws.loc[i,'artist'])\n",
    "no_spotify   = pd.DataFrame(columns = ['anime', 'season', 'year', 'category', 'songname', 'artist'])\n",
    "with_spotify = pd.DataFrame(columns = ['anime', 'season', 'year', 'category', 'songname', 'artist', 'spotify'])\n",
    "\n",
    "for i in sheet.index:\n",
    "    if sheet['spotify'][i] == ' ': no_spotify.loc[i] = sheet.loc[i]\n",
    "    else: with_spotify.loc[i] = sheet.loc[i]\n",
    "\n",
    "with_spotify = with_spotify.reset_index()\n",
    "del with_spotify['index']\n",
    "with_spotify['status'] = ''\n",
    "no_spotify = no_spotify.reset_index()\n",
    "del no_spotify['index']\n",
    "no_spotify['link'] = None\n",
    "with_spotify.to_csv('with_spotify.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f370a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with_spotify = pd.read_csv('with_spotify.csv')\n",
    "del with_spotify['Unnamed: 0']\n",
    "with_spotify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb861c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%%capture --no-display #hides warnings\n",
    "DownloadSpotify(with_spotify, 'F:/', 3)\n",
    "with_spotify.to_csv('with_spotify_updated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e13161",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with_spotify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840df499",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eed8396",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c6d541",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d50ea55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54856d24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd1ddef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13dc1f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d37c0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d4bbc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf9336a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5ce527",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23be7f2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58776c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313a7ad6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aaa488d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d3b2b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b238b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d168a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "songset = with_spotify\n",
    "i = 85\n",
    "sp_link = songset.loc[i,'spotify']\n",
    "year    = songset.loc[i,'year']\n",
    "season  = songset.loc[i,'season']\n",
    "anime   = songset.loc[i,'anime']\n",
    "oped    = songset.loc[i,'category']\n",
    "title   = songset.loc[i,'songname']\n",
    "artist  = songset.loc[i,'artist']\n",
    "path    = str(year) + '/' + str(season) + '/' + anime + '/' + oped + '/' + artist + '/'+ title\n",
    "\n",
    "for i in songset.index:\n",
    "        title   = songset.loc[i,'songname']\n",
    "        if len(title) >50:\n",
    "            print(i, title, len(title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49b4876",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "tables = testing2[0].find_all('td', {'width':'84%'})\n",
    "tablerows = tablerows.find_all('span ', attrs = {'class':'theme-song-index'})\n",
    "\n",
    "headings = []\n",
    "for td in tablerows[0].find_all('td'):\n",
    "    #remove any newlines and extra spaces from left and right\n",
    "    headings.append(td.b.text.replace('\\n', ' ').strip())\n",
    "#print(headings)\n",
    "\n",
    "a=tables[1].next.next.next\n",
    "a\n",
    "#######################\n",
    "Songsettitle = testing2[2].find_all('span',{'class':'theme-song-title'})\n",
    "outputSongsetartist = testing2[2].find_all('span',{'class':'theme-song-artist'})\n",
    "\n",
    "#https://stackoverflow.com/questions/49515530/how-to-convert-bs4-element-tag-to-pandas\n",
    "sst = pd.Series([i.text.strip('\\\"') for i in Songsettitle])\n",
    "ssa = pd.Series([i.text for i in Songsetartist])\n",
    "\n",
    "#https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html\n",
    "Songset = pd.concat([sst,ssa], axis=1)\n",
    "Songset\n",
    "#######################\n",
    "text = []\n",
    "for x in tables[1]:\n",
    "    if isinstance(x, bs4.element.NavigableString):\n",
    "        text.append(x.string.strip())\n",
    "print(' '.join(text))\n",
    "text\n",
    "text[0].strip('\\\"'')\n",
    "#######################\n",
    "soup = bs('''\n",
    "    <html>\n",
    "        <h2> Heading 2 <h2> Heading 1 </h2></h2>\n",
    "        <h2> Heading 1 </h2>\n",
    "    </html>\n",
    "    ''', 'lxml')\n",
    "# Get the whole h2 tag\n",
    "tag = soup.h2\n",
    "# Get the string inside the tag and convert \n",
    "# it into string\n",
    "text = []\n",
    "for x in tag:\n",
    "    if isinstance(x, bs4.element.NavigableString):\n",
    "        text.append(x.strip())\n",
    "print(' '.join(text))\n",
    "text\n",
    "#######################\n",
    "y = entry_page.find_all('div', {'class':'spaceit_pad'})\n",
    "for spaceit in y:\n",
    "    darktext = spaceit.find_all('span', {'class':'dark_text'})\n",
    "    for entry in darktext:\n",
    "        if entry.contents == 'Premiered:':\n",
    "        print(entry)\n",
    "#######################\n",
    "for i in saa:\n",
    "    if 'https' in i:\n",
    "        print(1)\n",
    "    else:\n",
    "        print(temp)\n",
    "        temp = []\n",
    "    temp.extend(i)\n",
    "#######################    \n",
    "'''\n",
    "animelink = 'https://myanimelist.net/anime/37206'\n",
    "html = requests.get(animelink).text\n",
    "entry_page = bs(html, 'lxml')\n",
    "\n",
    "for i in filtered_mal.index:\n",
    "    if filtered_mal['AnimeTitle'][i] == 'Gyo':\n",
    "        print(i)\n",
    "    \n",
    "filtered_mal.loc[269]\n",
    "\n",
    "for i in range(1206,1241):\n",
    "    #with_spotify['anime'][i] = 'M' + with_spotify['anime'][i]\n",
    "    #with_spotify['anime'][i] = with_spotify['anime'][i].lstrip('A')\n",
    "    print(with_spotify['anime'][i])\n",
    "'''\n",
    "#######################\n",
    "i = 0        \n",
    "year   = songset2.loc[i,'year']\n",
    "season = songset2.loc[i,'season']\n",
    "anime  = songset2.loc[i,'anime']\n",
    "oped   = songset2.loc[i,'category']\n",
    "title  = songset2.loc[i,'songname']\n",
    "artist = songset2.loc[i,'artist']\n",
    "path   = str(year) + '/' + season + '/' + anime + '/' + oped + '/' + artist + '/'+ title\n",
    "\n",
    "first_result = yts.VideosSearch(title + ' ' + artist, limit = 2)\n",
    "yt_link      = first_result.result()['result'][0]['link']\n",
    "\n",
    "yt = YouTube(yt_link)\n",
    "audio = yt.streams.get_audio_only()\n",
    "print(yt.title, yt.thumbnail_url)\n",
    "\n",
    "try: \n",
    "    audio.download(output_path = path) \n",
    "except: \n",
    "    print('Download Error) \n",
    "    \n",
    "file_name = audio.default_filename\n",
    "#source = path + '/' + file_name\n",
    "#if ' ' in file_name:\n",
    "#    os.rename(source, source.replace(' ', '_'))\n",
    "#file_name = source.replace(' ','_')\n",
    "\n",
    "file_without_ext = os.path.splitext(file_name)[0]\n",
    "command = f'ffmpeg -i {file_name} {file_without_ext}.mp3'\n",
    "os.system(command)\n",
    "#######################\n",
    "songset2 = with_spotify\n",
    "pd.set_option('display.max_columns', 7)\n",
    "pd.reset_option('max_columns')\n",
    "songset2\n",
    "#######################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8ef0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
